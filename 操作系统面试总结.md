# 操作系统面试总结

[toc]



## 进程/线程

### 进程

进程：**系统进行资源调度和分配的最小独立单位**。具有独立性，动态性，并发性，异步性。实现了os的并发性

### 线程

线程：**程序执行的最小单位**。自身基本不拥有系统资源，只拥有一些运行中必不可少的资源，[如程](https://www.nowcoder.com/jump/super-jump/word?word=如程)序计数器、寄存器和栈等。可与同属于一个进程的不同线程共享进程所拥有的全部资源。实现了进程间的并发性

### 进程与线程的联系与区别

区别：**资源（进程有独立的地址空间，线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间）、切换效率（进程切换时，耗费资源较大，效率要差一些）、通信机制**

**一个线程挂掉，会导致该线程所属的进程整个挂掉，进程中的其他线程也都挂掉，但是一个进程挂掉，不会影响其他进程**

### 进程间的通信方式

#### 管道

- **管道/匿名管道**：父子进程/兄弟进程之间的通信方式

> 没有名字的管道如何通信，当然是亲近的进程之间

- **有名管道**：任意进程间可以通信。严格遵守FIFO（先进先出）原则，并以磁盘文件的方式存在

> 有名字了，都可以联系了

#### 消息队列

- 消息的链表。既然是队列当然也服从FIFO（先进先出原则）

> 消息队列克服了信号承载信息量少的问题，信号只是一个简单的通知

#### 信号

- **（signal）**：用于通知接受进程某个事件发生了

> “给一个信号”呗，代表发生了什么

#### 信号量

1. 信号量是一个**计数器**，用于多进程对共享数据的访问
2. 信号量的目的：进程间的同步
3. 信号量的通信方式：用于解决与同步相关的问题并避免发生竞争关系

#### 共享内存

- **（sharing memory）**：进程间最有用的通信方式

> 1. 使得多进程可以访问同一块内存空间
> 2. 不同进程可以及时看到对方进程对共享数据的操作
> 3. 共享内存需要依赖某种同步操作（例互斥锁、信号量）

#### 套接字

- **（Sockets）**：TCP/IP通信方式，客户端与服务器端进程之间的网络通信。简单的说：不同主机进程之间用套接字中的相关函数完成通信过程

> 传输层的主体就是进程

#### 总结

​    所有的以上的方式都是生命周期随内核，不手动释就不会消失。

### 线程间的通信方式

针对于python中的threading模块来说

#### 全局变量

#### 消息队列 （threading模块中的Queue类）

#### 锁机制：包括互斥锁、条件变量、读写锁

互斥锁提供了以排他方式防止数据结构被并发修改的方法。
读写锁允许多个线程同时读共享数据，而对写操作是互斥的。
条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。

#### 信号量机制(Semaphore)

包括无名线程信号量和命名线程信号量

#### 信号机制(Signal)

类似进程间的信号处理

线程间的通信目的主要是用于线程同步，所以线程没有像进程通信中的用于数据交换的通信机制。

## 常见锁机制（国科大并发课程）

### 自旋锁/空转锁（CPU不断检查锁是否可用）

#### TAS/TTAS

```java
public class AtomicBoolean { //java.util.concurrent.atomic
	boolean value;
	public boolean getAndSet(boolean newValue) { //交换
	boolean prior = value;
	value = newValue;
	return prior;
	}
}
TAS锁
class TASlock {
	AtomicBoolean state = new AtomicBoolean(false); //锁的状态
	void lock() {
		while (state.getAndSet(true)) {} //空转，直到获得锁（状态从假到真）
	}
	void unlock() {
		state.set(false); //释放锁
	}
}
```

下面把TAS锁升级为TTAS锁（Test-and-Test-and-Set Lock），其加锁过程分为两个阶段：
1、潜伏阶段：持续对寄存器进行读操作，直到锁“看起来”是空闲的；
2、突袭状态：执行加锁操作（getAndSet），如果失败，则回到潜伏阶段。
释放TTAS锁的过程与TAS锁相同。

```java
class TTASlock {
	AtomicBoolean state = new AtomicBoolean(false);
	void lock() {
		while (true) {
			while (state.get()) {} //空转读，直到锁被释放
			if (!state.getAndSet(true))
				return; //加锁成功
		}
	}
}
```

#### 回退锁

如果锁看起来是空闲的，但却没能获得，避免争用的好办法是回退一段时间，而不是立刻再次争用。争用是指多个线程试图同时获得锁。当线程发现锁空闲但却没能获得时，说明存在争用，在重试前先回退。为确保发生争用的并发线程不会进入锁步（lock-step）模式，线程回退的时长应该是随机的。当线程再次争用失败时，回退时长的上限应加倍。

```java
public class Backoff implements lock {
	public void lock() {
		int delay = MIN_DELAY; //回退时长的上限
		while (true) {
			while (state.get()) {} //等待锁空闲
			if (!lock.getAndSet(true))
				return; //获得锁	
			sleep(random() % delay); //随机回退一段时间
			if (delay < MAX_DELAY)
				delay = 2 * delay; //回退时长的上限加倍
}}}
```

#### Anderson 队列锁

有效解决争用的办法仍然是将单个锁变量扩充为锁数组或者队列，以资源换效率。最早的队列锁是Anderson提出的。Anderson Queue Lock是一个布尔数组flag构成的队列。每个线程对应flag数组中的一位，称为槽（slot）。如果flag[j]为T，那么槽j的线程可以获得锁。初始时，flag[0]为T（下图○a）。加锁时，线程首先调用getAndIncremental()进行排队，指定自己的slot，然后忙等在该slot上（下图○b），直到该slot的flag变为T（下图○c）。释放锁时，线程把自己的slot变为F，将下一个slot变为T（下图○c）。

![image-20210420230454316](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20210420230454316.png)

```java
class ALock implements Lock {
	boolean[] flags={true,false,…,false};
	AtomicInteger next = new AtomicInteger(0); //下一个等待slot
	ThreadLocal<Integer> mySlot;
	public lock() {
		mySlot = next.getAndIncrement(); //设置下一个等待slot
		while (!flags[mySlot % n]) {}; //空转，忙等在自己的slot上
		flags[mySlot % n] = false; //获得锁，占用中
	}
	public unlock() {
	flags[(mySlot+1) % n] = true; //下一个线程可用
	}
}
```

#### CLH队列锁

CLH队列锁是Craig、Landin和Hagersten合作提出的，解决了Anderson锁占用资源过多的缺点。CLH锁的队列与通常的队列不同，其链接是隐式的，由线程的局部变量和Qnode结点共同构成。Qnode结点记录了锁的状态。如果其状态域为true，则相应的线程要么已经获得锁，要么正在等待锁；如果该域为false，则相应的线程已经释放了锁。
初始化时，tail（队尾）指向Qnode对象（下图○a）。粉色线程申请锁时，创建其Qnode对象，并将其状态域置为true（下图○b）。随后，交换tail和其Qnode结点引用，使该Qnode结点成为新队尾，同时获取指向前驱Qnode结点的引用（即原队尾，下图○c）。线程忙等在前驱结点上。接下来，红色线程申请锁时，创建其Qnode对象，将其状态域置为true，交换tail和其Qnode结点引用（下图○d）。红色线程忙等在前驱结点上（即粉色线程创建的Qnode结点）。粉色线程释放锁时，修改其Qnode结点的状态域为false，该结点可以被红色线程将来复用。红色线程发现前驱结点的状态域变成false，即可获得锁（下图○e）。

![image-20210420230654187](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20210420230654187.png)

Alock锁的空间开销大。假设锁对象为每个线程预备的共享数据结构需要L大小的内存空间，那么N个线程访问同一个锁对象需要O(LN)大小的空间。CLH锁只需要O(L+N)。

```java
class Qnode {
	AtomicBoolean locked = new AtomicBoolean(true); //新加入的结点
}
class CLHLock implements Lock {
	AtomicReference<Qnode> tail = new Qnode(); //队尾
	tail.locked.set(false); //队尾初始时，锁可用
	ThreadLocal<Qnode> myNode = new Qnode(); //线程Qnode
	ThreadLocal<Qnode> myPred;
	public void lock() {
		myNode.locked.set(true);
		Qnode pred = tail.getAndSet(myNode); //加入队尾
		myPred.set(pred);
		while (pred.locked) {}; //空转在pred结点
	}
	public void unlock() {
		myNode.locked.set(false); //通知后继线程
		myNode.set(myPred.get()); //回收前驱Qnode结点，并复用
	}
}
```

#### MCS队列锁

MCS队列锁是Mellor-Crummey和Scott提出的，满足FIFO特性，只在本地内存单元空转，拥有常量级空间大小。看起来是一个很理想的spin锁（在不考虑abortable的情况下）。
MCS队列锁的链表是显式的，每个Qnode结点包含指向后继结点的next指针。初始化时，tail指向null结点。粉色线程申请锁时，发现tail指向null结点，表明没有前驱结点（下图○a）。粉色线程直接获得锁，并将其Qnode结点加入队尾（下图○b）。
![image-20210420230933975](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20210420230933975.png)
接下来，红色线程申请锁，发现tail指向非null结点，创建其Qnode结点，将其状态域置为false，并将该Qnode加入队尾（修改原队尾指向结点的next指针指向该Qnode结点，然后忙等在此Qnode结点上，直到前驱线程修改此Qnode结点的状态域为false。粉色线程释放锁时，把后继结点的状态域置为false，红色线程即获得锁，粉色线程的Qnode结点可回收复用。

![image-20210420230941144](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20210420230941144.png)

```java
class Qnode {
	boolean locked = false;
	Qnode next = null;
}
class MCSLock implements Lock {
	tail = new AtomicReference<Qnode>(null);
	public void lock() {
		Qnode Qnode = new Qnode(); //新结点
		Qnode pred = tail.getAndSet(Qnode); //加入队尾
		if (pred != null) { //若队列不空
			Qnode.locked = true; //准备空转
			pred.next = Qnode; //将前驱结点的next指向新结点
			while (Qnode.locked) {} //在新结点上空转
	}}
	public void unlock() {
		if (Qnode.next == null) {
			if (tail.CAS(Qnode, null) //没有后继线程
				return;
			while (Qnode.next == null) {} //等待后继结点加入队尾
		}
		Qnode.next.locked = false; //通知后继结点
	}}
```


注：在unlock()方法中，if (tail.CAS(Qnode, null) return;一句非常重要。它检查当前线程的Qnode结点是否是tail指向的最后一个结点。如果是，那么tail直接指向null结点，队列为空；否则，表明后继线程正在将其Qnode结点加入队尾，但这个链接过程还未完成！这时，调用unlock()的线程（解锁线程）需要忙等待，直到链接完成。然后，解锁线程将后继Qnode结点的状态域置为false，完成锁的释放。

#### 超时锁

懒得弄了

### 管程锁

管程把阻塞锁定义为条件对象，包含以下方法：
1. void await()：线程休眠，进入条件对象定义的等待区。

2. void signal()：唤醒相应等待区内的一个线程。

3. void signalAll(): 唤醒相应等待区内的所有线程。

  当线程调用signal()时，该线程应已在管程内；而当线程被signal()唤醒时，将尝试进入管程。但唤醒者A与被唤醒者B之间只能有一个线程占用该管程。唤醒线程有两种方式处理：

  1. 非阻塞条件（Signal and Continue，Mesa style）：唤醒者A继续执行，直至该线程释放锁。然后，被唤醒者B竞争进入管程。
  2. 阻塞条件（Signal and Wait，Hoare-style）：唤醒者A进入等待区，直到被唤醒者B释放锁。然后，唤醒者A竞争进入管程。

  下图说明管程的工作流程：（1）线程申请锁；（2）线程获得锁，进入管程；（3）线程调用await()，释放锁并进入管程；（4）线程被唤醒，获得锁，返回管程；（5）线程释放锁，退出管程。需要说明的是，在线程释放锁退出管程后，不仅Wait Set里被唤醒的线程可以竞争申请锁，在Entry Set里的线程同样可以竞争申请锁，最终由哪个线程获得锁是不确定的。

![image-20210420231634277](C:\Users\Sakura\AppData\Roaming\Typora\typora-user-images\image-20210420231634277.png)

```java
Class LockedQueue<T> {
	final Lock lock = new ReentrantLock();
	final Condition notFull = lock.newCondition(); //创建条件对象
	final Condition notEmpty = lock.newCondition();
	final T[] items;
	int tail, head, count; …
}
public LockedQueue(int capacity) {
	items = (T[]) new Object[capacity];
}
public void enq(T x) {
	lock.lock();
	try {
		while (count == items.length)
			notFull.await(); //满则等待
		Enqueue x;
		++count;
		notEmpty.signal(); //唤醒等待notEmpty的线程
	} finally {
		lock.unlock();
	}
}
public T deq() {
	lock.lock();
	try {
		while (count == 0)
			notEmpty.await(); //空则等待
		Dequeue the first element;
		--count;
		notFull.signal(); //唤醒等待notFull的线程
		Return the element dequeued;
	} finally {
		lock.unlock();
	}
}
```

### 读写锁（一次只有一个写者，多个读者）

许多共享对象都有如下特性：大多数是读访问，即只返回对象的状态而不修改对象；只有少数是写访问，即会修改对象的状态。读-写锁允许多个读者并发访问。读-写锁的特性：

- 任一线程持有读锁或写锁时，其它线程不能获得写锁；
- 任一线程持有写锁时，其它线程不能获得读锁；除此之外，多个线程可获得多个读锁；

```java
public class SimpleReadWriteLock implements ReadWriteLock {
	int readers; //记录有多少读者，writer == false且readers == 0时可获得写锁
	boolean writer; //记录是否有写者，writer == false时可获得读锁
	Lock lock; //同步所有的锁
	Lock readLock, writeLock; //读锁，写锁
	Condition condition; //条件对象，与lock关联
	public SimpleReadWriteLock() {
		writer = false; readers = 0;
		lock = new ReentrantLock();
		readLock = new ReadLock(); writeLock = new WriteLock();
		condition = lock.newCondition(); //条件对象，与lock关联
	}
class ReadLock implements Lock { //lock() 和 unlock()只能由内部类访问
	public void lock() {
		lock.lock();
		try {
			while (writer) //等待释放写锁
			condition.await();
			readers++; //获得读锁，readers计数器加1
		} finally {
			lock.unlock();
		}
	}
	public void unlock() {
		lock.lock();
		try {
			readers--; //释放读锁，readers计数器减1
			if (readers == 0) //唤醒等待condition的所有线程
				condition.signalAll();
		} finally {
			lock.unlock();
		}
	}
class WriteLock implements Lock {
	public void lock() {
		lock.lock();
		try {
			while (writer || readers != 0)
				condition.await();
			writer = true;
		} finally {
			lock.unlock();
		}
	}
	public void unlock() {
		writer = false;
		condition.signalAll();
	}
}
}
```

公平读写锁（Fair Readers-Writers Lock）赋予写以优先级，保证一旦有写申请（写）锁，则不允许更多的读者获取（读）锁，只允许读者释放（读）锁，直到写者获得（写）锁为止。

```java
private class ReadLock implements Lock {
	public void lock() {
		lock.lock();
		try {
			while (writer)
				condition.await();
			readAcquires++;
		} finally {
			lock.unlock();
		}
	}
	public void unlock() {
		lock.lock();
		try {
			readReleases++;
			if (readAcquires == readReleases)
				condition.signalAll();
		} finally {
			lock.unlock();
		}
	}
}
private class WriteLock implements Lock {
	public void lock() {
		lock.lock();
		try {
			while (writer) //等待写者释放
				condition.await();
			writer = true;
			while (readAcquires != readReleases) //等待读者释放
				condition.await();
		} finally {
			lock.unlock();
		}
	}
	public void unlock() {
		writer = false;
		condition.signalAll();
	}
}
```

### 可重入锁（允许一个进程/线程多次拿到锁）

可重入锁是指一个线程可以在不释放锁的情况下，重复申请并获得该锁。这样，线程在递归或重复调用时不会陷入死锁。可重入锁的实现在本质上是使用一个计数器来记录线程获得锁的次数。

```java
public class SimpleReentrantLock implements Lock {
	Lock lock;
	Condition condition;
	int owner, holdCount;
	public SimpleReentrantLock() {
		lock = new SimpleLock();
		condition = lock.newCondition();
		owner = 0;
		holdCount = 0;
	}
	public void lock() {
		int me = ThreadID.get();
		lock.lock();
		try {
			if (owner == me) {
				holdCount++;
				return;
			}
			while (holdCount != 0)
				condition.await();
			owner = me;
			holdCount = 1;
		} finally {
			lock.unlock()
		}
	}
	public void unlock() {
		lock.lock();
		try {
			if (holdCount == 0 || owner != ThreadID.get())
				throw new IllegalMonitorStateException();
			holdCount--;
			if (holdCount == 0) {
				condition.signal();
			}
		} finally {
			lock.unlock();
		}
	}
}
```

### 信号量（一次允许多个线程操作锁对象）

互斥锁保证在任何时刻至多有一个线程进入临界区。信号量是互斥锁的一般形式，允许最多capacity个线程进入临界区。capacity是信号量的容量，在初始化时确定。信号量本身是一个原子计数器，记录进入临界区的线程数。Hoare已证明信号量机制和管程在理论上是等价的[1]。

- acquire()：P操作，请求进入临界区。
-  release()：V操作，离开临界区。

```java
public class Semaphore {
	final int capacity;
	int state;
	Lock lock;
	Condition condition;
	public Semaphore(int c) {
		capacity = c; //信号量容量
		state = 0;
		lock = new ReentrantLock();
		condition = lock.newCondition();
	}
	public void acquire() {
		lock.lock();
		try {
			while (state == capacity) //信号量满
				condition.await();
			state++;
		} finally {
			lock.unlock();
		}
	}
	public void release() {
		lock.lock();
		try {
			state--;
			condition.signalAll(); //唤醒等待线程
		} finally {
			lock.unlock();
		}
	}
}
```

### 粗粒度锁

以下都以链表为例

粗粒度同步的具体方法是：首先构造该数据结构的顺序实现，然后增加一个可扩展的锁域，并要求每个方法调用在开始时须获得该锁，在返回时须释放该锁。粗粒度锁实现的是最简单的并发，其正确性也易于保证。

### 细粒度锁

细粒度同步（Fine-Grained Synchronization）：不再使用单一锁来同步对共享对象的所有并发访问，而是将对象分解为一些独立的同步组件，分别用独立的锁来保护，以保证多个线程能够并发访问不同的同步组件。只有当多个线程试图并发访问同一组件时才需要用锁进行冲突管理（对象的分解）。

### 乐观锁

许多类似链表、树这样的对象是由多个组件通过引用链接而成的。访问这些并发对象的基本流程是按照引用的顺序结构依次访问，直到发现目标组件。线程会调用一些方法（如加入、删除等）对并发对象进行修改，但更多调用的是查找，不改变并发对象的状态。一种减少细粒度锁代价的方法是在查找时无需先获得锁。仅当发现目标组件时，才锁定组件，然后确认该组件在被找到和被锁定期间没有发生任何变化。如果确定没有发生变化，则查找成功，返回正确的结果，否则此次查找失败，只能重新查找。这种方法只有在成功次数多于失败次数时才有价值，所以称为乐观同步。（加锁的时机）

### 惰性锁

惰性同步（Lazy Synchronization）：把访问对象的方法进行分解，将其中复杂的工作推迟。典型的例子是把删除方法分解为逻辑删除和物理删除两步。从数据结构中删除元素时，先通过设置标志位表示该元素已被删除（即逻辑删除），然后才真正从数据结构中删除该元素（即物理删除）。逻辑删除这一步可以与其它方法并发执行，而物理删除这一步的并发度低（需要加锁），被推迟执行。（方法的分解）

### 无阻塞锁CAS

充分调动并发对象的并发性，完全消除锁，仅在必要时采用CAS（compareAndSet）等原子操作实现同步。实际上，该方法把锁的粒度减小至原子寄存器（即共享内存的基础），这已经是最高的并发度了。但是，其实现也最为复杂，如果底层CAS操作的代价较大，有时不一定能获得最好的性能。（最细粒度的同步）

## 死锁

多个进行相互等待对方资源，在得到所有资源继续运行之前，都不会释放自己已有的资源，这样造成了循环等待的现象，称为死锁。

### 死锁产生的四个必要条件

- **资源互斥条件：**资源是独占的且排他使用，进程互斥使用资源，即任意时刻一个资源只能给一个进程使用，其他进程若申请一个资源，而该资源被另一进程占有时，则申请者等待直到资源被占有者释放。

- **资源不可剥夺条件：**进程所获得的资源在未使用完毕之前，不被其他进程强行剥夺，而只能由获得该资源的进程资源释放。

- **占有和等待/请求和保持条件：**进程每次申请它所需要的一部分资源，在申请新的资源的同时，继续占用已分配到的资源。

- **循环等待条件：**在发生死锁时必然存在一个进程等待队列{P1,P2,…,Pn},其中P1等待P2占有的资源，P2等待P3占有的资源，…，Pn等待P1占有的资源，形成一个进程等待环路，环路中每一个进程所占有的资源同时被另一个申请，也就是前一个进程占有后一个进程所深情地资源。

  以上给出了导致死锁的四个必要条件，只要系统发生死锁则以上四个条件至少有一个成立。事实上**循环等待**的成立蕴含了前三个条件的成立，似乎没有必要列出然而考虑这些条件对死锁的预防是有利的，因为可以通过破坏四个条件中的任何一个来预防死锁的发生。

### 死锁预防

我们可以通过破坏死锁产生的4个必要条件来 预防死锁，由于资源互斥是资源使用的固有特性是无法改变的。

主要的思想：提前破坏死锁产生的四个必要条件，静态地避免死锁发生

1. **破坏互斥条件**

   - 把某些互斥访问的资源改造成共享资源（SPOOLing技术）

   - 缺点：可行性不高

2. **破坏“不可剥夺”条件：**一个进程不能获得所需要的全部资源时便处于等待状态，等待期间他占有的资源将被隐式的释放重新加入到 系统的资源列表中，可以被其他的进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行。

   - 方法一：申请的资源得不到满足时则立即释放当前所拥有的全部资源
   - 方法二：由操作系统干预按照优先级从别的进程那里剥夺某些资源
   - 缺点：实现复杂；强行剥夺可能会导致进程失效；反复申请和释放导致系统性能较低；可能导致饥饿

3. **破坏”请求与保持条件“：**第一种方法静态分配即每个进程在开始执行时就申请他所需要的全部资源。第二种是动态分配即每个进程在申请所需要的资源时他本身不占用系统资源。

   - 方法一：在进程执行前，一次性分配进程所需的全部资源，如果资源得不到满足，则不分配任何资源，暂不执行。

   - 方法二：只有当进程不占有资源时才分配给进程资源。进程可以占有一部分资源，但是当它向os索取更多资源的时候必须先释放当前占有的全部资源

   - 缺点：进程动态执行，难以事先预知进程所需的全部资源；资源利用率低；可能会导致饥饿

4. **破坏“循环等待”条件：**采用资源有序分配其基本思想是将系统中的所有资源顺序编号，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。

   - 给**所有资源[排序](https://www.nowcoder.com/jump/super-jump/word?word=排序)编号**，所有进程对资源的申请必须按照**严格递增**的顺序提出，每次只能申请序号更大的资源，防止产生环路

   - 缺点：导致资源浪费，不方便加入新资源

### 死锁避免

死锁避免的基本思想：系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，如果分配后系统可能发生死锁，则不予分配，否则予以分配，这是一种保证系统不进入死锁状态的动态策略。
如果操作系统能保证所有进程在有限时间内得到需要的全部资源，则系统处于安全状态否则系统是不安全的。

**银行家[算法]()**

步骤：

1.一个进程向系统提出资源申请，首先检查此次申请是否超过了之前声明的最大需求数，如果超过则认为出错；

2.检查此时系统剩余的可用资源是否还能够满足该次请求，若不满足则等待

3.试探着（不是真的分配）分配，更改各类数据结构

4.用安全性[算法]()检查此次资源分配是否会导致系统进入不安全状态，若安全才进行资源分配

总结：进程提出资源请求时，判断是否超出进程最大需求资源、是否超出系统最大所剩资源，然后模拟分配，用安全[算法]()检测分配了之后是否所有线程属于安全序列，如果安全则才能正式分配资源。

> **安全[算法]()**
>
> 检查当前的剩余可用资源是否能满足某个进程的最大需求，如果可以，就将该进程加入到安全序列，并把该进程持有的资源全部回收，重复上述过程，看最终是否能够让所有进程加入安全序列。

**系统处于安全状态一定不会死锁**

**系统处于不安全状态未必会死锁，但是死锁时系统一定是不安全状态**

1. **安全状态**是指：如果系统存在 由所有的安全序列{P1，P2，…Pn},则系统处于安全状态。一个进程序列是安全的，如果对其中每一个进程Pi(i >=1 && i <= n)他以后尚需要的资源不超过系统当前剩余资源量与所有进程Pj(j < i)当前占有资源量之和，系统处于安全状态则不会发生死锁。
2. **不安全状态**：*如果不存在任何一个安全序列，则系统处于不安全状态*。他们之间的对对应关系如下图所示：
   ![这里写图片描述](https://img-blog.csdn.net/20170411180038465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvanl5MzA1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

### 死锁检测

1、画出资源分配图

系统死锁，可利用资源分配图来描述。如下图所示，用长方形代表一个进程，用框代表一类资源。由于一种类型的资源可能有多个，用框中的一个点代表一类资源中的一个资源。从进程到资源的有向边叫请求边，表示该进程申请一个单位的该类资源；从资源到进程的边叫分配边，表示该类资源已经有一个资源被分配给了该进程。

 

![img](https://img-blog.csdn.net/20180731103654409?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pnbTIwNDc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![img](https://img-blog.csdn.net/20180731103816331?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pnbTIwNDc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

 

2、简化资源分配图

第一步：先看A资源，它有三个箭头是向外的，因此它一共给进程分配了3个资源，此时，A没有空闲的资源剩余。

第二步：再看B资源，它有一个箭头是向外的，因此它一共给进程分配了1个资源，此时，B还剩余一个空闲的资源没分配。 

第三步：看完资源，再来看进程，先看进程P2，它只申请一个A资源，但此时A资源已经用光了，所以，进程P2进入阻塞状态，因此，进程P2暂时不能化成孤立的点。 

第四步：再看进程P1，它只申请一个B资源，此时，系统还剩余一个B资源没分配，因此，可以满足P1的申请。这样，进程P1便得到了它的全部所需资源，所以它不会进入阻塞状态，可以一直运行，等它运行完后，我们再把它的所有的资源释放。相当于：可以把P1的所有的边去掉，变成一个孤立的点，如下图所示：

 

![img](https://img-blog.csdn.net/20180731103850412?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pnbTIwNDc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

第五步：进程P1运行完后，释放其所占有的资源（2个A资源和1个B资源），系统回收这些资源后，空闲的资源便变成2个A资源和1个B资源，由于进程P2一直在申请一个A资源，所以此时，系统能满足它的申请。这样，进程P2便得到了它的全部所需资源，所以它不会进入阻塞状态，可以一直运行，等它运行完后，我们再把它的所有的资源释放。相当于：可以把P2的所有的边都去掉，化成一个孤立的点，变成下图： 

![img](https://img-blog.csdn.net/20180731103912949?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pnbTIwNDc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

（若能消去图中所有的边，则称该图是**可完全简化**的，如上图）



### 死锁解除

**①资源剥夺法**

挂起某些死锁资源，抢占其资源分配给其他的死锁进程

缺点：被挂起的进程可能会导致饥饿

**②终止进程法**

强制终止部分、甚至全部死锁进程，并剥夺其所占有的资源

缺点：付出代价较大，有些进程已近结束，终止就会功亏一篑

**③进程回退法**

让一个或多个进程回退到不会发生死锁的地步

缺点：要求系统记录进程的历史信息，设置还原点，浪费系统资源





## 内存换出算法

### 总述

　　为提高内存利用率，解决内存供不应求的问题，更加合理的使用内存，人们创造了**分页式内存抽象**。同时有一个**虚拟内存**的概念，是指将内存中暂时不需要的部分写入硬盘，看上去硬盘扩展了内存的容量，所以叫做“虚拟”内存。使用虚拟内存，应用程序可以使用比实际物理内存更大的内存空间。可以认为这个更大的内存空间就在硬盘上，只有将某一部分需要被用到时，才被写入真实内存；当它暂时不再被用到时，又被写回硬盘。分页式内存管理将物理内存分为等大的小块，每块大小通常为1K、2K、4K等，称为**页帧；**逻辑内存（使用虚拟内存技术扩大的内存，可认为其位于硬盘上）也被分为等大的小块，称为**页**；且**页和页帧**的大小一定是一样的，它是写入真实内存和写回硬盘最小单位。

　　介绍另外几个概念：

　　**使用位**：每个页帧都有一个使用位，记录此页帧是否被使用。

　　**修改位（脏位）**：每个页帧都有一个脏位，记录此页帧是否被更改。调出真实内存时，被更改过的页帧要写回硬盘，未被更改过的页帧直接扔掉即可，因为硬盘上此页帧的副本仍然有效。

　　**逻辑地址**：使用虚拟内存技术扩大后的内存的地址。

　　**物理地址**：真实内存的地址。

　　当然，进程载入到真实内存才可以运行，而进程代码使用的是逻辑地址，所以牵扯到一个**地址转换**的问题，将逻辑地址转换为物理地址。逻辑地址可分为两段，前半段代表页号，后半段代表页内偏移，物理地址也可分为两段，前半段代表页帧号，后半段代表页内偏移。地址转换的方法即，将逻辑地址的页号对应为物理地址的页帧号（对应关系记录在一张表中，比如页号为5，对应到真实内存中页帧号为3），页内偏移不同变化（页和页帧的大小是一样的）。

### **FIFO先进先出** 

　FIFO算法的思想很简单，就是置换出当前已经待在内存里时间最长的那个页。FIFO算法的运行速度很快，不需要考虑其他的因素，需要的开销很少。但是正是由于没有考虑页面的重要性的问题，FIFO算法很容易将重要的页换出内存。

思想：淘汰最先进入的页面，采用队列实现，先进先出

缺点：可能会频繁地换入换出，影响效率

### **Optimal最佳算法**

它需要知道以后要被用到的页，然后将不会被用到的页换出内存；如果所有页都会被用到，就把需要使用时间离现在最长的页换出，以尽量使不好的情况晚发生。这种方法能使系统获得最佳性能，但是，它是不可能实现的......因为当前无法获知以后哪些页要被用到。不过最优算法还是能够作为其他算法优秀程度的衡量。

思想：淘汰以后不需要使用或者最远才会用到的页

缺点：实际操作中无法预测未来页的使用情况

### **LRU最久未使用淘汰算法**

为获得对最优算法的模拟，提出了LRU算法。由于当前时间之后需要用到哪些页无法提前获知，于是记录当前时间之前页面的使用情况，认为之前使用过的页面以后还会被用到。在置换时，将最近使用最少的页面换出内存。此种方法的开销比较大。

思想：淘汰最长时间没有被使用的页。软件可以用**双向链表**实现，访问的时候就把该页移到头部，淘汰时淘汰尾部。

### **LFU最不经常使用淘汰算法**

思想：淘汰访问频率最小的页，以次数为参考。新加入的页放在末尾，计数器置为1，每次访问计数器+1，并重新按照计数器大小[排序]()，淘汰计数器最小的页

### Second Chance（第二次机会）算法

　　为了避免FIFO算法将重要的页换出内存，Second Chance算法提供了一些改进。Second Chance算法在将页面换出内存前检查其使用位（使用位前文有介绍），如果其使用位为1，证明此页最近有被使用，猜测它还可能被使用，于是不把它置换出内存，但是把其使用位置为0，随后检查下一个页面，直到发现某页的使用位为0，将此页置换出内存。

### Clock算法（时钟轮转法）

　　为了节约Second Chance算法一个接着一个检查使用位的开销，时钟轮转法又提出了改进。时钟轮转法将所有的页组成一个圆，圆心的指针指向下一个要被置换的页面，置换前同样检查使用位，如果使用位为1，同样将其使用位置为0，随后将顺指针旋转，检查下一个页面，直到发现某页的使用位为0，将此页置换出内存。很容易理解此算法为什么叫“时钟”轮转法。

图示：

![img](https://images2015.cnblogs.com/blog/730938/201511/730938-20151115195530290-388030166.jpg)

　　此时2号页是下一个要被置换出内存的页，置换时如果发现其使用位为1，则将使用位置0后顺时针旋转指针检查1号页。

### NRU（Not Recent Used， 最近未使用）算法

　　前面提到修改位和使用位，NRU算法利用这两个标志位将所有页帧分为4组：

　　第0组：修改位和使用位都为0；

　　第1组：修改位为0，使用位为1；

　　第2组：修改位为1，使用位为0；

　　第3组：修改位和使用位都为1。

　　NRU算法从组数最小的一组中随机选择一个页面将其移出内存。可能有人会发现第2组这种情况根本不会出现，如果一个页帧被修改，其修改位会被置1，同时它也被使用了，其使用位也会被置1；即不会出现被修改但是没有被使用的情况。真实情况是，页帧的使用位会被定时清零，这样第3组经过一次清零就会变成第2组。这也符合“最近”未使用，即很久以前被使用的页帧被清零了，不在统计范围内，只要“最近”没有被使用，就很有可能被移出。

　　NRU算法不是最好的，但是它使用起来开销很小，用较小的代价就得到了不错的效果，不失为一种不错的算法。



## 进程调度算法

调度算法是指：根据系统的资源分配策略所规定的资源分配算法

### 先来先服务FCFS

先到的进程先调度，执行过程不会被中断直到进程结束。

优点：易于实现，且相当公平。

缺点：比较有利于长进程，而不利于短进程。

在进程调度中，FCFS调度算法每次从就绪队列中选择最先进入该队列的进程，将处理机分配给它，使之投入运行，直到完成或因某种原因而阻塞时才释放处理机。

 下面通过一个实例来说明FCFS调度算法的性能。假设系统中有4个作业，它们的提交时间分别是8、8.4、8.8、9，运行时间依次是2、1、0.5、0.2，系统釆用FCFS调度算法，这组作业的平均等待时间、平均周转时间和平均带权周转时间如下表所示。

| 作业号 | 提交时间 | 运行时间 | 开始时间 | 等待时间 | 完成时间 | 周转时间 | 带权周转时间 |
| ------ | -------- | -------- | -------- | -------- | -------- | -------- | ------------ |
| 1      | 8        | 2        | 8        | 0        | 10       | 2        | 1            |
| 2      | 8.4      | 1        | 10       | 1.6      | 11       | 2.6      | 2.6          |
| 3      | 8.8      | 0.5      | 11       | 2.2      | 11.5     | 2.7      | 5.4          |
| 4      | 9        | 0.2      | 11.5     | 2.5      | 11.7     | 2.7      | 13.5         |

> 平均等待时间 t = (0+1.6+2.2+2.5)/4=1.575
> 平均周转时间 T = (2+2.6+2.7+2.7)/4=2.5
> 平均带权周转时间 W = (1+2.6+5.牡13.5)/4=5.625

 FCFS调度算法属于不可剥夺算法。从表面上看，它对所有作业都是公平的，但若一个长作业先到达系统，就会使后面许多短作业等待很长时间，因此它不能作为分时系统和实时系统的主要调度策略。但它常被结合在其他调度策略中使用。例如，在使用优先级作为调度策略的系统中，往往对多个具有相同优先级的进程按FCFS原则处理。

 FCFS调度算法的特点是算法简单，但效率低；对长作业比较有利，但对短作业不利（相对SJF和高响应比）；有利于CPU繁忙型作业，而不利于I/O繁忙型作业。



### 短作业优先

优先分配给短进程执行。

优点：平均周转时间最短，进程等待时间缩短，可以增大系统吞吐量。

缺点：难以准确预估进程执行时间，开销较大；不利于长进程，有可能“饥饿”现象。

 短作业优先调度算法是一个非抢占策略，他的原则是下一次选择预计处理时间最短的进程，因此短进程将会越过长作业，跳至队列头。该算法即可用于作业调度，也可用于进程调度。但是他对长作业不利，不能保证紧迫性作业（进程）被及时处理，作业的长短只是被估算出来的。

**缺点：**

- 该算法对长作业不利，SJF调度算法中长作业的周转时间会增加。更严重的是，如果有一长作业进入系统的后备队列，由于调度程序总是优先调度那些 (即使是后进来的）短作业，将导致长作业长期不被调度（“饥饿”现象，注意区分“死锁”。后者是系统环形等待，前者是调度策略问题）。
- 该算法完全未考虑作业的紧迫程度，因而不能保证紧迫性作业会被及时处理。
- 由于作业的长短只是根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。

> 【注意】 SJF调度算法的平均等待时间、平均周转时间最少。

### 高响应比优先

一种关于先来先服务和短作业优先的折中算法，当一个长进程等待时间过长，就会获得较高的优先权，因此不会出现“饥饿”现象。

优先级D=（执行时间+等待时间）/执行时间

优点：不会出现“饥饿”现象，长作业也有机会被调度。

缺点：每次都需要计算优先级，系统开销大。

根据比率：R=(w+s)/s （R为响应比，w为等待处理的时间，s为预计的服务时间）

　　如果该进程被立即调用，则R值等于归一化周转时间（周转时间和服务时间的比率）。R最小值为1.0，只有第一个进入系统的进程才能达到该值。调度规则为：当前进程完成或被阻塞时，选择R值最大的就绪进程，它说明了进程的年龄。当偏向短作业时，长进程由于得不到服务，等待时间不断增加，从而增加比值，最终在竞争中赢了短进程。和最短进程优先、最短剩余时间优先一样，使用最高响应比策略需要估计预计服务时间。

 高响应比优先调度算法主要用于作业调度，该算法是对FCFS调度算法和SJF调度算法的一种综合平衡，同时考虑每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。

根据公式可知：

- 当作业的等待时间相同时，则要求服务时间越短，其响应比越高，有利于短作业。
- 当要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高，因而它实现的是先来先服务。
- 对于长作业，作业的响应比可以随等待时间的增加而提高，当其等待时间足够长时，其响应比便可升到很高，从而也可获得处理机。克服了饥饿状态，兼顾了长作业。

### 时间片轮转法

为进程设定时间片，即每个进程运行的时间，在一个时间片结束时，发生时钟中断，调度程序暂停执行并加入队尾，通过上下文切换执行当前队首进程

优点：算法简单，响应时间短。

缺点：不利于处理紧急作业；时间片过小会导致频繁进程上下文切换，增大系统开销；时间片过长则会退化为FCFS。

> 时间片轮转调度算法主要适用于分时系统。在这种算法中，系统将所有就绪进程按到达时间的先后次序排成一个队列，进程调度程序总是选择就绪队列中第一个进程执行，即先来先服务的原则，但仅能运行一个时间片，如100ms。在使用完一个时间片后，即使进程并未完成其运行，它也必须释放出（被剥夺）处理机给下一个就绪的进程，而被剥夺的进程返回到就绪队列的末尾重新排队，等候再次运行。

 在时间片轮转调度算法中，时间片的大小对系统性能的影响很大。如果时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则时间片轮转调度算法就退化为先来先服务调度算法。如果时间片很小，那么处理机将在进程间过于频繁切换，使处理机的开销增大，而真正用于运行用户进程的时间将减少。因此时间片的大小应选择适当。

 时间片的长短通常由以下因素确定：系统的响应时间、就绪队列中的进程数目和系统的处理能力。



### 优先级调度算法

优先级调度算法又称优先权调度算法，该算法既可以用于作业调度，也可以用于进程调度，该算法中的优先级用于描述作业运行的紧迫程度。

在作业调度中，优先级调度算法每次从后备作业队列中选择优先级最髙的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。在进程调度中，优先级调度算法每次从就绪队列中选择优先级最高的进程，将处理机分配给它，使之投入运行。

根据新的更高优先级进程能否抢占正在执行的进程，可将该调度算法分为：

- 非剥夺式优先级调度算法。当某一个进程正在处理机上运行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在运行的进程继续运行，直到由于其自身的原因而主动让出处理机时（任务完成或等待事件），才把处理机分配给更为重要或紧迫的进程。
- 剥夺式优先级调度算法。当一个进程正在处理机上运行时，若有某个更为重要或紧迫的进程进入就绪队列，则立即暂停正在运行的进程，将处理机分配给更重要或紧迫的进程。

而根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种：

- 静态优先级。优先级是在创建进程时确定的，且在进程的整个运行期间保持不变。确定静态优先级的主要依据有进程类型、进程对资源的要求、用户要求。
- 动态优先级。在进程运行过程中，根据进程情况的变化动态调整优先级。动态调整优先级的主要依据为进程占有CPU时间的长短、就绪进程等待CPU时间的长短。

### 多级反馈队列

>  分为多个队列，**优先级从高到低**，不同的队列**分配的时间片由小到大**
>
>  进程到来先将其加入第一队列，按照FCFS的原则给第一队列的进程分配时间片，时间片耗尽进程未处理完毕则将其加入到下一队列的尾部（本身就是最后一个队列的话就加入到最后一个队列的尾部）。只有当1~n-1队列为空时才能开始处理第n个队列。

多级反馈队列算法，不必事先知道各种进程所需要执行的时间，他是当前被公认的一种较好的进程调度算法。

多级反馈队列调度算法的实现思想如下：

1. 应设置多个就绪队列，并为各个队列赋予不同的优先级，第1级队列的优先级最高，第2级队列次之，其余队列的优先级逐次降低。
2. 赋予各个队列中进程执行时间片的大小也各不相同，在优先级越高的队列中，每个进程的运行时间片就越小。例如，第2级队列的时间片要比第1级队列的时间片长一倍， ……第i+1级队列的时间片要比第i级队列的时间片长一倍。
3. 当一个新进程进入内存后，首先将它放入第1级队列的末尾，按FCFS原则排队等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第2级队列的末尾，再同样地按FCFS 原则等待调度执行；如果它在第2级队列中运行一个时间片后仍未完成，再以同样的方法放入第3级队列……如此下去，当一个长进程从第1级队列依次降到第 n 级队列后，在第 n 级队列中便釆用时间片轮转的方式运行。
4. 仅当第1级队列为空时，调度程序才调度第2级队列中的进程运行；仅当第1 ~ (i-1)级队列均为空时，才会调度第i级队列中的进程运行。如果处理机正在执行第i级队列中的某进程时，又有新进程进入优先级较高的队列（第 1 ~ (i-1)中的任何一个队列），则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i级队列的末尾，把处理机分配给新到的更高优先级的进程。

多级反馈队列的优势有：

- - 终端型作业用户：短作业优先。
    - 短批处理作业用户：周转时间较短。
    - 长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。

### **实际情况中如何选择进程调度算法**

根据是否要关注优先权，如果不考虑优先权可以采用高响应比优先[算法](https://www.nowcoder.com/jump/super-jump/word?word=算法)，如果考虑优先权可以采用多级反馈队列。

